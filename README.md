# LLM-Reasoning-Error-Taxonomy

Understanding How Large Language Models Fail at Reasoning
A structured analysis of reasoning failure modes across LLMs, focusing on interpretability, reliability, and human-aligned evaluation.


## Key Contributions
- A novel taxonomy of LLM reasoning errors
- Task-specific prompt design for failure analysis
- Cross-model evaluation framework
- Human-centered analysis of reasoning reliability

## Motivation
Understanding reasoning failures is critical for building trustworthy and reliable AI systems.
This project emphasizes interpretability, robustness, and responsible evaluation.

## Tasks Covered
- Multi-step mathematical reasoning
- Logical deduction
- Instruction following
